#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Sep  4 16:57:35 2020

@author: winkleram
"""

import os
import sys
import argparse
import json
import pandas as pd
import glob
from shutil import copyfile
from natsort import natsorted, ns
import nibabel as nib
import re
import hashlib
import string
import random

def printHelp(argv, description): # ===========================================

    # Print help. This is meant to be called from parseArguments
    print(description)
    print("")
    print("Usage:")
    print("{} -n <dirin> -b <dirout>".format(argv[0]))
    print("")
    print("-n : Input directory from NIDB export, with .nii.gz and .json files.")
    print("-b : Output BIDS directory.")
    print("-r : Renumber entities as 1, 2, 3, etc.")
    print("-s : Use study name as subject name.")
    print("")
    print("_____________________________________")
    print("Anderson M. Winkler")
    print("UTRGV")
    print("First version: Sep/2020")
    print("This version:  May/2025")
    print("http://brainder.org")
    exit(1)
    
def parseArguments(argv): # ===================================================

    # Parse arguments
    description = "Convert the input NIDB directory into BIDS."
    if len (argv) <= 1:
        printHelp(argv, description)
    else:
        epilog = "Run without arguments for basic usage information."
        parser = argparse.ArgumentParser(description=description, epilog=epilog)
        parser.add_argument('-n', '--nidb',     help="Input NIDB directory.",
                            type=str, dest='dirin', action='store', required=False)
        parser.add_argument('-b', '--bids',     help="Output BIDS directory.",
                            type=str, dest='dirout', action='store', required=True)
        parser.add_argument('-s', '--ses2sub',  help="Use study name as subject name.",
                            dest='ses2sub', action='store_true', required=False)
        parser.add_argument('-r', '--renumber', help="Re-number sessions, runs and echos to 1, 2, etc...",
                            dest='renumber', action='store_true', required=False)
        args = parser.parse_args(argv[1:])
        return args

def readjson(jsonfile): # =====================================================
    with open(jsonfile, 'r') as fp:
        J = json.load(fp)
    return J

def writejson(J, jsonfile): # =================================================
    if os.path.exists(jsonfile):
        link2copy(jsonfile)
    with open(jsonfile, 'w') as fp:
        json.dump(J, fp, indent=2)
    return

def readhdr(niftifile): # =====================================================
    nii = nib.load(niftifile)
    dim = nii.header.get_data_shape()
    pixdim = nii.header.get_zooms()
    return dim, pixdim

def link2copy(file): # ========================================================
    # Replaces a hard link for an actual copy of the file
    nlinks = os.stat(file).st_nlink
    if nlinks > 1:
        pth, nam = os.path.split(file)
        tmpfile = os.path.join(pth, '{}.tmp'.format(nam))
        copyfile(file, tmpfile)
        os.remove(file)
        os.rename(tmpfile, file)
    return

def isbidsfile(filename): # ===================================================
    fnam, fext = os.path.splitext(filename)
    if fext == '.gz':
        fnam, fext = os.path.splitext(fnam)
        fext = fext + '.gz'
    if fext in ['.json', '.nii', '.nii.gz', '.bvec', '.bval', '.tsv']:
        isit = True
    else:
        isit = False
    fnam = os.path.basename(fnam)
    return isit, fnam, fext

def simplifystring(S):  # =====================================================
    special_chars = [' ', '-', '_', '.', '+', '(', ')', '/', '=']
    for c in special_chars:
        S = S.replace(c, '')
    S = S.lower()
    return S

def cleanentity(curdir, f, entity='run'): # ==================================
    # Drop redundant entities such as 'run' or 'echo', and rename
    # them to sequential numbers where they're not dropped.
    fnam, fext = os.path.splitext(f)
    if fext == '.gz':
        fnam, fext = os.path.splitext(fnam)
        fext = fext + '.gz'
    ftok = fnam.split('_')
    for idxt, t in enumerate(ftok):
        if t.startswith('{}-'.format(entity)):
            ftok[idxt] = '{}-*'.format(entity)
    flist = glob.glob(os.path.join(curdir, '_'.join(ftok) + fext))
    flist.sort()
    if isinstance(flist, list):
        if len(flist) == 1 and entity != 'ses': # note the exception for ses, as we always need a ses-1
            for idxt, t in enumerate(ftok):
                if t.startswith('{}-'.format(entity)):
                    ftok.pop(idxt)
        else:
            idxf = flist.index(os.path.join(curdir, fnam + fext)) + 1
            for idxt, t in enumerate(ftok):
                if t.startswith('{}-'.format(entity)):
                    ftok[idxt] = '{}-{}'.format(entity, idxf)
    newfnam = '_'.join(ftok)
    oldfile = os.path.join(curdir, fnam + fext)
    newfile = os.path.join(curdir, newfnam + fext)
    return oldfile, newfile

def studyincomments(jsonfile, studynam): # ====================================
    J = readjson(jsonfile)
    if 'ImageComments' in J and 'Session:' in J['ImageComments']:
        if not J['ImageComments'].endswith(studynam):
            tok = J['ImageComments'].split(';')
            for i,t in enumerate(tok):
                if t.startswith('Session:'):
                    tok[i] = 'Session:{}'.format(studynam)
            J['ImageComments'] = ';'.join(tok)
            writejson(J, jsonfile)
    return J

def addtotalreadouttime(jsonfile): # ==========================================
    J = readjson(jsonfile)
    if 'TotalReadOutTime' not in J:
        J['TotalReadOutTime'] = J['VendorReportedEchoSpacing'] * \
            (J['AcquisitionMatrixPE']/J['ParallelReductionFactorInPlane'] - 1)
    writejson(J, jsonfile)

def md5file(filepath): # ======================================================
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def random_string(k=10): # ====================================================
    chars = string.ascii_letters + string.digits  # A-Z, a-z, 0-9
    return ''.join(random.choice(chars) for _ in range(k))


# =============================================================================
#   MAIN FUNCTION
# =============================================================================

# Parse arguments
#argv = './gobs2bids.py -n dups_nidb -b dups_bids'.split(' ')
#args = parseArguments(argv)
args = parseArguments(sys.argv)

# Note that dirin (-n nidbdir) is optional. If not supplied, and if the BIDS
# directory exists and -r was supplied, it will renumber entities in the bidsdir.
if args.dirin != None:
    
    # Create output directory if it does not exist
    if os.path.isdir(args.dirout):
        print('Error: Output directory already exists: {}'.format(args.dirout))
        sys.exit()
    else:
        os.makedirs(args.dirout)
    
    # Get a mapping between NIDB UID and AltUIDs
    print('Mapping between NIDB UID and AltUIDs')
    slist = next(os.walk(args.dirin))[1]
    slist.sort()
    altuid = []
    nidb   = []
    for sid in slist:
        for curdir, subdirs, files in os.walk(os.path.join(args.dirin, sid)):
            for f in files:
                nid = f.split('_')[0]
                if nid not in nidb:
                    nidb.append(nid)
                    altuid.append(sid)
    
    # Populate output directory with hard links to the original files
    S = pd.DataFrame(columns=['olddir'])
    for curdir, subdirs, files in os.walk(args.dirin):
        for f in files:
            isfbids, oldfnam, fext = isbidsfile(f)
            if isfbids:
                print('{} -> {}'.format(os.path.join(curdir, f), os.path.join(args.dirout, f)))
                if not os.path.isfile(os.path.join(args.dirout, f)):
                    print('Creating link: {} -> {}'.format(os.path.join(curdir, f), os.path.join(args.dirout, f)))
                    os.link(os.path.join(curdir, f), os.path.join(args.dirout, f))
                    S.loc[oldfnam,'olddir'] = curdir
                else:
                    print('Target already exists: {} -> {}'.format(os.path.join(curdir, f), os.path.join(args.dirout, f)))
                #os.chmod(os.path.join(args.dirout, f), 0o660)
    
    D = {}
    # For each file in the newly created BIDS dir:
    for curdir, subdirs, files in os.walk(args.dirout):
        
        # For each file in the current directory
        for f in natsorted(files, alg=ns.IGNORECASE):
            
            # Check if the current file could belong to BIDS and if it's a JSON
            isfbids, oldfnam, fext = isbidsfile(f)
            if isfbids and fext == '.json':
                
                # This is the NIDB ID and a few other info, from the file names:
                nidb_id, studynum, seriesnum, serialnum = oldfnam.split('_')
                studynam = ''
                oldfnamp = S.loc[oldfnam,'olddir'].split('/')
                if nidb_id in oldfnamp:
                    studynam = oldfnamp[oldfnamp.index(nidb_id)+1]
                
                # Create dictionary entries to store all stuff
                if nidb_id not in D:
                    D[nidb_id] = {}
                if studynum not in D[nidb_id]:
                    D[nidb_id][studynum] = pd.DataFrame(columns=[
                            'nidb_id','studynam','seriesnum','serialnum','series_description',
                            'acquisition_date','acquisition_time','echo_time','newfnam','datatype',
                            'B0_identifier','B0_source','img_md5','duplicated'])
        
                # Start populating the fields
                D[nidb_id][studynum].loc[oldfnam,'nidb_id']   = nidb_id # redundant but gives extra flexibility later
                D[nidb_id][studynum].loc[oldfnam,'studynam']  = studynam
                D[nidb_id][studynum].loc[oldfnam,'seriesnum'] = seriesnum
                D[nidb_id][studynum].loc[oldfnam,'serialnum'] = serialnum
                                
                # Read the JSON and collect as much information as needed. These pieces
                # will be used for renaming later:
                J = readjson(os.path.join(args.dirout, f))
                series_description = ''
                acquisition_date   = ''
                acquisition_time   = ''
                echo_time          = ''
                if 'SeriesDescription' in J:
                    series_description = simplifystring(J['SeriesDescription'])
                    D[nidb_id][studynum].loc[oldfnam,'series_description'] = series_description
                if 'AcquisitionDateTime' in J:
                    acquisition_date, acquisition_time = J['AcquisitionDateTime'].split('T')
                    acquisition_date = acquisition_date.replace('-','')
                    acquisition_time = acquisition_time.replace(':','')
                    D[nidb_id][studynum].loc[oldfnam,'acquisition_date'] = acquisition_date
                    D[nidb_id][studynum].loc[oldfnam,'acquisition_time'] = acquisition_time
                if 'EchoTime' in J:
                    #echo_time = '{0:.4f}'.format(J['EchoTime']).replace('0.', '')
                    echo_time = J['EchoTime']
                    D[nidb_id][studynum].loc[oldfnam,'echo_time'] = echo_time
                if 'ScanOptions' in J:
                    scan_options = J['ScanOptions']
                    D[nidb_id][studynum].loc[oldfnam,'scan_options'] = scan_options
                
                # Read the NIFTI file and collect some information from the header
                dim, pixdim = readhdr(os.path.join(args.dirout, f).replace('.json','.nii.gz'))
                D[nidb_id][studynum].loc[oldfnam,'dimi']    = dim[0]
                D[nidb_id][studynum].loc[oldfnam,'dimj']    = dim[1]
                D[nidb_id][studynum].loc[oldfnam,'dimk']    = dim[2]
                D[nidb_id][studynum].loc[oldfnam,'pixdimi'] = round(pixdim[0]*100)/100
                D[nidb_id][studynum].loc[oldfnam,'pixdimj'] = round(pixdim[1]*100)/100
                D[nidb_id][studynum].loc[oldfnam,'pixdimk'] = round(pixdim[2]*100)/100
                
                # Now prepare to rename according to the type of file
                if args.ses2sub:
                    sub_id = D[nidb_id][studynum].loc[oldfnam,'studynam']
                else:
                    sub_id = D[nidb_id][studynum].loc[oldfnam,'nidb_id']
                substr   = 'sub-{}'.format(sub_id) # subject ID (AltUID)
                sesstr   = '' # string to store the session number
                taskstr  = '' # string sto store the task name
                runstr   = '' # string to store the run number
                datatype = '' # string to store the datatypeectory for this type of image
                acqstr   = '' # string to store type of acquisition
                recstr   = '' # string to store the kind of reconstruction
                dirstr   = '' # string to store the direction of phase enconding
                echstr   = '' # strong to store the echo time
                modstr   = '' # string to store the type of modality
                if acquisition_date != '':
                    sesstr = '_ses-{}'.format(acquisition_date)
                if acquisition_time != '':
                    runstr = '_run-{}'.format(acquisition_time)
                B0_identifier = ''
                B0_source     = ''
                multi_echo    = False
    
                # ===== ANATOMY ===============================================
                print('Simplified SeriesDescription: {}'.format(series_description))
                if 'mpraget1ax08mmti' in series_description: # ----------------
                    # This is a T1w
                    datatype = 'anat'
                    modstr   = '_T1w'
                    if   series_description.endswith('ti766'):
                        acqstr = '_acq-TI766'
                    elif series_description.endswith('ti773'):
                        acqstr = '_acq-TI773'
                    elif series_description.endswith('ti780'):
                        acqstr = '_acq-TI780'
                    elif series_description.endswith('ti787'):
                        acqstr = '_acq-TI787'
                    elif series_description.endswith('ti794'):
                        acqstr = '_acq-TI794'
                    elif series_description.endswith('ti801'):
                        acqstr = '_acq-TI801'
                    elif series_description.endswith('ti808'):
                        acqstr = '_acq-TI808'
                    
                elif 't1mpr08isohcptr2400' in series_description: # -----------
                    # This is a T1w
                    datatype = 'anat'
                    modstr   = '_T1w'
                    
                elif 'flair' in series_description: # -------------------------
                    # This is a FLAIR
                    datatype = 'anat'
                    modstr = '_FLAIR'
                    
                # ===== FIELDMAPS =============================================
                    
                elif 'fieldmapping' in series_description: # ------------------
                    # These are distortion correction scans
                    datatype = 'fmap'
                    if   'M' in J['ImageType']:
                        modstr     = '_magnitude'
                        multi_echo = True
                    elif 'P' in J['ImageType']:
                        modstr     = '_phasediff'
                    B0_identifier = 'fieldmapping'
                    
                elif 'topup' in series_description and \
                     'bold'  in series_description: # -------------------------
                    datatype  = 'fmap'
                    modstr    = '_epi'
                    PEdir = J['PhaseEncodingDirection']
                    if   PEdir.endswith('-'):
                        PEdir = PEdir.replace('-','neg')
                    elif PEdir.endswith('+'):
                        PEdir = PEdir.replace('+','pos')
                    else:
                        PEdir = PEdir + 'pos'
                    dirstr = '_dir-{}'.format(PEdir)
                    B0_identifier = 'fmri_topup'
                    
                elif 'diffmb32mmb00'   in series_description or \
                     'diffmb32mmb0180' in series_description: # ---------------
                    datatype  = 'fmap'
                    modstr    = '_epi'
                    PEdir = J['PhaseEncodingDirection']
                    if   PEdir.endswith('-'):
                        PEdir = PEdir.replace('-','neg')
                    elif PEdir.endswith('+'):
                        PEdir = PEdir.replace('+','pos')
                    else:
                        PEdir = PEdir + 'pos'
                    dirstr = '_dir-{}'.format(PEdir)
                    B0_identifier = 'dwi_topup'

                elif 'diffmb12mmb00'   in series_description or \
                     'diffmb12mmb0180' in series_description: # ---------------
                    datatype  = 'fmap'
                    modstr    = '_epi'
                    PEdir = J['PhaseEncodingDirection']
                    if   PEdir.endswith('-'):
                        PEdir = PEdir.replace('-','neg')
                    elif PEdir.endswith('+'):
                        PEdir = PEdir.replace('+','pos')
                    else:
                        PEdir = PEdir + 'pos'
                    dirstr = '_dir-{}'.format(PEdir)
                    B0_identifier = 'dwi_topup'

                # ===== RESTING STATE =========================================
                
                elif 'boldepi128' in series_description: # --------------------
                    # This is resting state FMRI 
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-rest'
                    multi_echo = False
                    B0_source  = 'fieldmapping'
                
                elif 'ep2dboldlong' in series_description: # ------------------
                    # This is resting state FMRI 
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-longrest'
                    multi_echo = False
                    B0_source  = 'fieldmapping'
                    
                elif 'ep2dbold' in series_description: # ----------------------
                    # This is resting state FMRI 
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-rest'
                    multi_echo = False
                    B0_source  = 'fieldmapping'
                
                elif 'mocoseries' == series_description: # --------------------
                    # This is resting state FMRI, motion corrected 
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-rest'
                    recstr     = '_rec-moco'
                    multi_echo = False
                    B0_source  = 'fieldmapping'
                
                elif 'boldcheck' in series_description: # ---------------------
                    # This is resting state FMRI (IGAB) SBREF?
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-rest'
                    multi_echo = False
                    acqstr     = '_acq-check'
                    B0_source  = 'fieldmapping'
                    
                elif 'boldmb324mm' in series_description: # -------------------
                    # This is resting state FMRI (IGAB)
                    datatype   = 'func'
                    modstr     = '_bold'
                    taskstr    = '_task-rest'
                    multi_echo = False
                    B0_source  = 'fieldmapping'
                    
                # ===== DIFFUSION =============================================
                
                elif series_description.endswith('highresdti') or \
                     series_description.endswith('diffusionhighres'): # -------
                    # This is diffusion
                    datatype   = 'dwi'
                    modstr     = '_dwi'
                    taskstr    = ''
                    multi_echo = False
                    B0_source  = None
                    
                elif 'diffmb32mmb700064' == series_description or \
                     'diffmb12mmb700064' == series_description: # -------------
                    # This is diffusion
                    datatype   = 'dwi'
                    modstr     = '_dwi'
                    taskstr    = ''
                    multi_echo = False
                    B0_source  = 'dwi_topup'
                
                else: # -------------------------------------------------------
                    print('Skipping: {}{} ({})'.format(oldfnam, fext, series_description))
                    datatype   = 'unknown'
                    modstr     = '_unknown'
                    taskstr    = ''
                    multi_echo = False
                    raise
                
                # For multi-echo data, get the echo time
                if datatype == 'func' and multi_echo and echo_time != '':
                    echstr = '_echo-{}'.format(echo_time)
                
                # Populate the main dict for later
                newfnam = '{}{}{}{}{}{}{}{}{}'.format(substr, sesstr, acqstr, taskstr, recstr, dirstr, runstr, echstr, modstr)
                D[nidb_id][studynum].loc[oldfnam,'datatype']      = datatype
                D[nidb_id][studynum].loc[oldfnam,'B0_identifier'] = B0_identifier
                D[nidb_id][studynum].loc[oldfnam,'B0_source']     = B0_source
                D[nidb_id][studynum].loc[oldfnam,'newfnam']       = newfnam
                print('Provisional filenames: {} -> {}'.format(oldfnam, newfnam))
    
    # Make small edits to the json files:
    # 1) Ensure that studynam is correct in the json file. This helps to solve 
    #    issues with studies "corrected" in the scanner console and re-exported 
    #    to the database, sometimes creating duplicates with different study names
    # 2) For BOLD scans, ensure TotalReadOutTime is present
    print('Making small edits or additions to json files (not altering original files)')
    for nidb_id in D:
        for studynum in D[nidb_id]:
            for oldfnam in D[nidb_id][studynum].index.tolist():
                studyincomments(os.path.join(curdir, '{}.json'.format(oldfnam)),
                                D[nidb_id][studynum].loc[oldfnam,'studynam'])
                if D[nidb_id][studynum].loc[oldfnam,'datatype'] == 'func':
                    addtotalreadouttime(os.path.join(curdir, '{}.json'.format(oldfnam)))
    
    # Compute the hashes for the image files with duplicate names, to find duplicate images
    for nidb_id in D:
        for studynum in D[nidb_id]:
            D[nidb_id][studynum]['duplicated'] = False
            dupnames = D[nidb_id][studynum].duplicated(subset=['newfnam'], keep=False)
            for oldfnam in D[nidb_id][studynum].index.tolist():
                if dupnames[oldfnam]:
                    print('Computing hash for: {}'.format(oldfnam))
                    D[nidb_id][studynum].loc[oldfnam,'img_md5'] = md5file(os.path.join(curdir, '{}.nii.gz'.format(oldfnam)))
                else:
                    D[nidb_id][studynum].loc[oldfnam,'img_md5'] = random_string(k=10)
            D[nidb_id][studynum]['duplicated'] = D[nidb_id][studynum].duplicated(subset=['img_md5'], keep='first')
    
    # Deal with magnitude fieldmap naming
    for nidb_id in D:
        for studynum in D[nidb_id]:
            idxdup = D[nidb_id][studynum]['duplicated']
            newfname_orig = D[nidb_id][studynum]['newfnam'][~idxdup].copy()
            mi = 1
            for oldfnam in D[nidb_id][studynum][~idxdup].index.tolist():
                ma = sum(newfname_orig == D[nidb_id][studynum].loc[oldfnam,'newfnam'])
                if D[nidb_id][studynum].loc[oldfnam,'newfnam'].endswith('_magnitude'):
                    D[nidb_id][studynum].loc[oldfnam,'newfnam'] = D[nidb_id][studynum].loc[oldfnam,'newfnam'] + '{}'.format(mi)
                    mi += 1
                    if mi == ma + 1:
                        mi = 1

    # Deal with identical new filenames
    for nidb_id in D:
        for studynum in D[nidb_id]:
            idxdup = D[nidb_id][studynum]['duplicated']
            newfname_orig = D[nidb_id][studynum]['newfnam'][~idxdup].copy()
            mi = 1
            for oldfnam in D[nidb_id][studynum][~idxdup].index.tolist():
                ma = sum(newfname_orig == D[nidb_id][studynum].loc[oldfnam,'newfnam'])
                if ma > 1:
                    tok = D[nidb_id][studynum].loc[oldfnam,'newfnam'].split('_')
                    for t in tok:
                        if t.startswith('run-'):
                            D[nidb_id][studynum].loc[oldfnam,'newfnam'] = D[nidb_id][studynum].loc[oldfnam,'newfnam'].replace(t, '{}x{}'.format(t, mi))
                    mi = mi + 1
                if mi == ma + 1:
                    mi = 1
    
    # Add EchoTime1 and EchoTime2 to the phasediff field maps
    for nidb_id in D:
        for studynum in D[nidb_id]:
            idxdup = D[nidb_id][studynum]['duplicated']
            EchoTimes = {}
            for oldfnam in D[nidb_id][studynum][~idxdup].index.tolist():
                if D[nidb_id][studynum].loc[oldfnam,'datatype'] == 'fmap' and \
                    D[nidb_id][studynum].loc[oldfnam,'newfnam'] != None and \
                    re.search(r'_magnitude\d$', D[nidb_id][studynum].loc[oldfnam,'newfnam']):
                    EchoTimes['EchoTime{}'.format(D[nidb_id][studynum].loc[oldfnam,'newfnam'][-1])] = D[nidb_id][studynum].loc[oldfnam,'echo_time']
                elif D[nidb_id][studynum].loc[oldfnam,'datatype'] == 'fmap' and \
                    D[nidb_id][studynum].loc[oldfnam,'newfnam'] != None and \
                    D[nidb_id][studynum].loc[oldfnam,'newfnam'].endswith('_phasediff'):
                    J = readjson(os.path.join(curdir, '{}.json'.format(oldfnam)))
                    for et in EchoTimes:
                        J[et] = EchoTimes[et]
                    print('Adding EchoTimes to: {}'.format('{}.json'.format(oldfnam)))
                    writejson(J, os.path.join(curdir, '{}.json'.format(oldfnam)))
                    EchoTimes = {}
                    
    # Sort out the fieldmaps (add B0Field* and IntendedFor fields to the JSON)
    for nidb_id in D:
        for studynum in D[nidb_id]:
            idxdup = D[nidb_id][studynum]['duplicated']
            for oldfnam in D[nidb_id][studynum][~idxdup].index.tolist():
                
                # Field maps proper (B0FieldIdentifier)
                if D[nidb_id][studynum][~idxdup].loc[oldfnam,'B0_identifier'] != '':
                    print('Adding B0FieldIdentifier and IntendedFor to: {}'.format('{}.json'.format(oldfnam)))
                    idx = D[nidb_id][studynum][~idxdup].loc[:,'B0_source'] == D[nidb_id][studynum][~idxdup].loc[oldfnam,'B0_identifier']
                    intendedfor = []
                    for newfnam in D[nidb_id][studynum][~idxdup].loc[idx,'newfnam']:
                        bids_path = os.path.join(newfnam.split('_')[0],
                                                 'ses-{}'.format(D[nidb_id][studynum][~idxdup].loc[oldfnam,'acquisition_date']), 
                                                 D[nidb_id][studynum][~idxdup].loc[oldfnam,'datatype'], 
                                                 '{}.nii.gz'.format(newfnam))
                        intendedfor.append('bids::{}'.format(bids_path))
                    intendedfor.sort()
                    J = readjson(os.path.join(curdir, '{}.json'.format(oldfnam)))
                    if 'IntendedFor' in J:
                        J['IntendedFor']   = J['IntendedFor'] + intendedfor
                    else:
                        J['IntendedFor']   = intendedfor
                    J['IntendedFor']       = list(set(J['IntendedFor']))
                    J['B0FieldIdentifier'] = D[nidb_id][studynum][~idxdup].loc[oldfnam,'B0_identifier']
                    writejson(J, os.path.join(curdir, '{}.json'.format(oldfnam)))
                    
                # Images that use the field maps as source
                if D[nidb_id][studynum][~idxdup].loc[oldfnam,'B0_source'] != '':
                    print('Adding B0FieldSource to: {}'.format('{}.json'.format(oldfnam)))
                    J = readjson(os.path.join(curdir, '{}.json'.format(oldfnam)))
                    J['B0FieldSource'] = D[nidb_id][studynum][~idxdup].loc[oldfnam,'B0_source']
                    writejson(J, os.path.join(curdir, '{}.json'.format(oldfnam)))
    
    # Rename the files, from the NIDB names to BIDS names, delete duplicates.
    for nidb_id in D:
        for studynum in D[nidb_id]:
            for oldfnam in D[nidb_id][studynum].index:
                if args.ses2sub:
                    sub_id = D[nidb_id][studynum].loc[oldfnam,'studynam']
                else:
                    sub_id = D[nidb_id][studynum].loc[oldfnam,'nidb_id']
                for iext in ['.json', '.nii.gz', '.bvec', '.bval', '.tsv']:
                    oldfile = os.path.join(args.dirout, '{}{}'.format(oldfnam, iext))
                    if os.path.isfile(oldfile):
                        if D[nidb_id][studynum].loc[oldfnam,'duplicated']:
                            os.remove(oldfile)
                        else:
                            newfile = os.path.join(args.dirout, 
                                                    'sub-{}'.format(sub_id),
                                                    'ses-{}'.format(D[nidb_id][studynum].loc[oldfnam,'acquisition_date']),
                                                    D[nidb_id][studynum].loc[oldfnam,'datatype'],
                                                    '{}{}'.format(D[nidb_id][studynum].loc[oldfnam,'newfnam'], iext))
                            newdir, newname = os.path.split(newfile)
                            if not os.path.isdir(newdir):
                                os.makedirs(newdir)
                            print('Renaming: {} -> {}'.format(oldfile, newfile))
                            os.rename(oldfile, newfile)

# Renumber ses, run, echo, to 1, 2, 3, etc.
# This part will only run if the "-r" option is given
if args.renumber:
    if os.path.isdir(args.dirout):
        # Cleanup echo and run numbers
        for entity in ['echo', 'run']:
            oldlist = []
            newlist = []
            for curdir, subdirs, files in os.walk(args.dirout):
                for f in sorted(files):
                    oldfile, newfile = cleanentity(curdir, f, entity=entity)
                    oldlist.append(oldfile)
                    newlist.append(newfile)
            for curdir, subdirs, files in os.walk(args.dirout):
                for f in sorted(files):
                    if curdir.endswith('fmap') and f.endswith('.json'):
                        funcdir = os.path.join(os.path.split(curdir)[0], 'func')
                        J = readjson(os.path.join(curdir, f))
                        if 'IntendedFor' in J:
                            for iidx, inam in enumerate(J['IntendedFor']):
                                ifdir, ifnam = os.path.split(inam)
                                ofnam = os.path.join(funcdir, ifnam)
                                if ofnam in oldlist:
                                    oidx = oldlist.index(ofnam)
                                    J['IntendedFor'][iidx] = os.path.join(ifdir, os.path.split(newlist[oidx])[-1])
                            writejson(J, os.path.join(curdir, f))
            for i in range(0, len(oldlist)):
                print('Renaming: {} -> {}'.format(oldlist[i],  newlist[i]))
                os.rename(oldlist[i], newlist[i])
    
        # Cleanup session numbers
        # Deal with the directories ses-* first
        olddirs = []
        newdirs = []
        for curdir, subdirs, files in os.walk(args.dirout, topdown=False):
            for d in sorted(subdirs):
                olddir, newdir = cleanentity(curdir, d, entity='ses')
                if newdir != olddir:
                    olddirs.append(olddir)
                    newdirs.append(newdir)
        for i in range(0, len(olddirs)):
            print('Renaming: {} -> {}'.format(olddirs[i], newdirs[i]))
            os.rename(olddirs[i], newdirs[i])
        # Then deal with the files
        oldlist = []
        newlist = []
        for curdir, subdirs, files in os.walk(args.dirout):
            dtok = curdir.split('/')
            for idxt, t in enumerate(dtok):
                if t.startswith('ses-'):
                    sesstr = t
            for f in sorted(files):
                ftok = f.split('_')
                for idxt, t in enumerate(ftok):
                    if t.startswith('ses-'):
                        ftok[idxt] = sesstr
                oldfile = os.path.join(curdir, f)
                newfile = os.path.join(curdir, '_'.join(ftok))
                if newfile != oldfile:
                    oldlist.append(oldfile)
                    newlist.append(newfile)
                    print('Renaming: {} -> {}'.format(oldfile, newfile))
                    os.rename(oldfile, newfile)
        # Now rename the 'IntendFor' inside the .json
        for curdir, subdirs, files in os.walk(args.dirout):
            for f in sorted(files):
                if curdir.endswith('fmap') and f.endswith('.json'):
                    dtok = curdir.split('/')
                    for idxt, t in enumerate(dtok):
                        if t.startswith('ses-'):
                            newsesstr = t
                    #funcdir = os.path.join(os.path.split(curdir)[0], 'func')
                    J = readjson(os.path.join(curdir, f))
                    if 'IntendedFor' in J:
                        for iidx, inam in enumerate(J['IntendedFor']):
                            ifdir, ifnam = os.path.split(inam)
                            dtok = ifdir.split('/')
                            for idxt, t in enumerate(dtok):
                                if t.startswith('ses-'):
                                    oldsesstr = t
                            J['IntendedFor'][iidx] = J['IntendedFor'][iidx].replace(oldsesstr, newsesstr)
                    writejson(J, os.path.join(curdir, f))
    else:
        print('Error: BIDS directory does not exist: {}'.format(args.dirout))
        sys.exit(1)

# Write dataset_description.json
J = { 'Name':'GOBS',
      'BIDSVersion': '1.0.0'}
writejson(J, os.path.join(args.dirout,'dataset_description.json'))
print('Finished.')
sys.exit(0)
